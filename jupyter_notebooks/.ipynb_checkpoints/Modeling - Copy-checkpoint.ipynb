{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: right;'><b>Data Scientist :</b> Ruslan S.</p>\n",
    "<p style='text-align: right;'><b>Collaborator :</b> Luka Anicin</p>\n",
    "</div>\n",
    "<h1 style='text-align: center;'>Sentiment Analysis - Natural Language Processing</h1>\n",
    "<h3>Steps : Data Wrangling, Exploratory Data Analysis (EDA)</h3>\n",
    "<p><b>Introduction: </b>Natural language processing (NLP) relates to the branch of computer science (artificial intelligence or AI) and is concerned with giving machines the ability to understand text and spoken words in a much similar way as human beings can.</p>  \n",
    "<p>Sentiment Analysis is the classification of people's feelings or expressions into different viewpoints. Sentiments could be Positive, Negative, Neutral, and so on. The process is done in different consumer-centered branches to investigate human opinions on a singular product or topic.</p>\n",
    "<img src='img/2.jpg'>\n",
    "<br><br><b>DATA STAGES:</b>\n",
    "<ul>\n",
    "    <li>Text iput</li>\n",
    "    <li>Tokenization</li>\n",
    "    <li>Stop Word Filtering</li>\n",
    "    <li>Negation</li>\n",
    "    <li>Stemming</li>\n",
    "    <li>Classification</li>\n",
    "    <li>Sentiment Class</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>IMPORT LIBRARIES/PACKAGES...</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rshul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk # It's a library that performs text processing tasks for Natural Language Processing\n",
    "nltk.download('stopwords') \n",
    "from nltk.corpus import stopwords # English words which does not add much meaning to a sentence\n",
    "from nltk.stem import SnowballStemmer # Stemmers remove morphological affixes from words, leaving only the word stem\n",
    "\n",
    "from wordcloud import WordCloud # It's a data visualization technique used for representing text data in which the size of each word indicates its frequency or importance\n",
    "\n",
    "from sklearn.model_selection import train_test_split # Split arrays or matrices into random train and test subsets\n",
    "from sklearn.preprocessing import LabelEncoder # Encode target labels with value between 0 and n_classes-1\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer # Text tokenization utility class\n",
    "from keras.preprocessing.sequence import pad_sequences # It makes all the sequence in one constant length\n",
    "\n",
    "# modeling part\n",
    "from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>DATA CLEANING...</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "path = \"../data/raw/raw.csv\"\n",
    "df = pd.read_csv(filepath_or_buffer=path, header=None, encoding='latin')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking data types of the columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns\n",
    "def renaming_df(df, col_names):\n",
    "    df.columns = col_names\n",
    "    print(df.head(3))\n",
    "    return df\n",
    "\n",
    "col_names = ['target', 'id', 'date', 'query', 'user_name', 'predictor']\n",
    "df = renaming_df(df, col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>NOTE: </b>For our task, we need only two columns: target (sentiment level) and predictor (text/values). Others columns could be dropped.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping unnecessary columns\n",
    "def drop_cols(df, col_names):\n",
    "    df = df.drop(labels=col_names, axis=1)\n",
    "    print(df.head(3))\n",
    "    return df\n",
    "\n",
    "col_names_drop = ['id', 'date', 'query', 'user_name']\n",
    "df = drop_cols(df, col_names_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking unique values for target column\n",
    "df['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing 'Zero' value with word 'Negative' and 'Four' value with word 'Positive'\n",
    "def col_substitution(df, col_name, dic_val):\n",
    "    df[col_name] = df[col_name].map(dic_val)\n",
    "    print(\"Unique values: \", df[col_name].unique())\n",
    "    print(df.head(5))\n",
    "    return df\n",
    "\n",
    "dic_val = {\n",
    "    0: 'Negative',\n",
    "    4: 'Positive',\n",
    "}\n",
    "df = col_substitution(df, 'target', dic_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing values\n",
    "print(df['target'].isnull().sum())\n",
    "print(df['predictor'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking duplicates\n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling duplicates\n",
    "def drop_duplicates_df(df):\n",
    "    df = df.drop_duplicates(ignore_index=True)\n",
    "    print(df.duplicated().sum())\n",
    "    return df\n",
    "\n",
    "df = drop_duplicates_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>NOTE:</b> Now we need to look at the distribution of our data. It's very important to have evenly divided data sets (classes) for avoiding any bias issues in future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzing destribuation\n",
    "sns.histplot(data=df, x='target', hue='target').set_title('Histogram of distributed target values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>NOTE:</b> As we can see we have pretty much-balanced class values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring predictor columns\n",
    "df['predictor'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>NOTE:</b> As we can see the predictor column has raw data (data that should be cleaned). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>PRE-PROCESSING...</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>NOTE:</b> Tweets consist of different types of values besides simple (relative) words. It could be hyperlinks, images, or punctuation marks. So, our work here is to remove all of this noise to make the prediction more accurate.</p>\n",
    "<img src=\"img/1.jpeg\" alt='Image cleaning'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here, we are going to apply stemming and lemmatization. Stemming is the process of reducing inflected words to their word stem, base, or root form generally a written word form. Where lemmatization is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.</p>\n",
    "<img src=\"img/3.png\" alt=\"StemmingAndLemmatization\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Additionally, we will be dealing with user ids and hyperlinks in our string values.</p>\n",
    "<img src=\"img/4.jpg\" alt=\"Link\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>And finally, we will complete our pre-process by removing stop words. Stop words are a set of commonly used words in a language. Examples of stop words in English are “a”, “the”, “is”, “are” and etc. Stop words are commonly used in Text Mining and Natural Language Processing (NLP) to eliminate words that are so commonly used that they carry very little useful information.</p>\n",
    "<img src=\"img/5.png\" alt='Stop words'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating instances for Stopwords and Stemming\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_stemmer = SnowballStemmer('english')\n",
    "snow_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular expression pattern\n",
    "re_patter = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_data(data, isStemming=False):\n",
    "    \"\"\"\n",
    "    Removing all noise.\n",
    "        data: the text\n",
    "        isStemming: the process of getting the normal forms of the words\n",
    "    \"\"\"\n",
    "    data = re.sub(re_patter, \" \", str(data).lower()).strip()\n",
    "    data = data.split()\n",
    "    token_list = []\n",
    "    for word in data:\n",
    "        if word not in stop_words:\n",
    "            if isStemming:\n",
    "                token_list.append(snow_stemmer.stem(word))\n",
    "            else:\n",
    "                token_list.append(word)\n",
    "    return \" \".join(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying pre-processing\n",
    "df['predictor'] = df['predictor'].apply(lambda x: cleaning_data(x))\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>VISUALIZATION...</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the ratio/frequencies of positive and negative words\n",
    "# link: https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html\n",
    "def show_word_cloud(class_val):\n",
    "    mask = df['target'] == class_val\n",
    "    predictor_values = \" \".join(df[mask]['predictor'])\n",
    "\n",
    "    max_words = 1800\n",
    "    width = 1300\n",
    "    height = 500\n",
    "    \n",
    "    word_cloud = WordCloud(max_words=max_words,\n",
    "                          width=width,\n",
    "                          height=height)\n",
    "    word_cloud = word_cloud.generate(predictor_values)\n",
    "    plt.imshow(word_cloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show positives\n",
    "show_word_cloud('Positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show negatives\n",
    "show_word_cloud('Negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TRAIN/TEST SPLIT...</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting our data\n",
    "# link: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "print(f\"Train size: {train_data.shape}\\nTest size: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TOKENIZATION...</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>NOTE: </b>Tokenization basically refers to splitting up a larger body of text into smaller lines, words or even creating words for a non-English language. The various tokenization functions in-built into the nltk module itself and can be used in programs as shown below.</p>\n",
    "<img src=\"img/6.jpg\" alt=\"tokenization\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying tokenization\n",
    "# link: https://www.kaggle.com/arunrk7/nlp-beginner-text-classification-using-lstm\n",
    "token_instance = Tokenizer()\n",
    "token_instance.fit_on_texts(train_data['predictor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordIndex = token_instance.word_index\n",
    "vocabSize = len(token_instance.word_index) + 1\n",
    "vocabSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_num = 10\n",
    "for key, val in wordIndex.items():\n",
    "    if temp_num > 0:\n",
    "        print(key, val)\n",
    "        temp_num -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pad_sequences to make the length of sequences the same. If it's too short adding pads, otherwise truncate it.\n",
    "# link: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "token_instance.texts_to_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 30\n",
    "\n",
    "def pad_trunc_funtion(data, col_name, max_length):\n",
    "    return pad_sequences(token_instance.texts_to_sequences(data[col_name]),\n",
    "                                     maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_trunc_funtion(train_data, 'predictor', MAX_SEQ_LENGTH)\n",
    "X_test = pad_trunc_funtion(test_data, 'predictor', MAX_SEQ_LENGTH)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>STOPPED HERE</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking\n",
    "X_train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating variable with unique target values (classes)\n",
    "classes = list(train_data['target'].unique())\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LABEL ENCODING...</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>NOTE: </b>Label Encoding is a popular encoding technique for handling categorical variables. In this technique, each label is assigned a unique integer based on alphabetical ordering.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying encoding and reshaping\n",
    "encoder = LabelEncoder().fit(list(train_data['target']))\n",
    "\n",
    "y_train = encoder.transform(list(train_data['target']))\n",
    "y_test = encoder.transform(list(test_data['target']))\n",
    "\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(y_train[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping \n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>WORD EMBEDDING...</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>NOTE: </b>Word Embedding is a language modeling technique used for mapping words to vectors of real numbers. It represents words or phrases in vector space with several dimensions. ... The input layer contains the context words and the output layer contains the current word.</p>\n",
    "<img src=\"img/7.png\" alt=\"word embedding\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to use GloVe: global vectors for word representation\n",
    "# link: https://nlp.stanford.edu/projects/glove/\n",
    "WORD_EMBEDDING = \"../temp/glove.6B/glove.6B.50d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_read_word_embedding(path_file):\n",
    "    counter = 0\n",
    "    word_vectors = {}\n",
    "    \n",
    "    with open(path_file, mode='r', encoding=\"utf8\") as we:\n",
    "        try:\n",
    "            for i in we:\n",
    "                list_of_values = i.split()\n",
    "                key = list_of_values[0]\n",
    "                coefs = np.asarray(list_of_values[1:], dtype='float32')\n",
    "                word_vectors[key] = coefs\n",
    "        except:\n",
    "            counter += 1\n",
    "            print(we)\n",
    "            \n",
    "    print(f\"There is/are {counter} lines with error.\")\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_idx = open_read_word_embedding(WORD_EMBEDDING)\n",
    "print(len(embedding_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSION = 50\n",
    "embedding_mtx = np.zeros((vocabSize, EMBEDDING_DIMENSION))\n",
    "print(embedding_mtx.shape)\n",
    "embedding_mtx[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for key, value in wordIndex.items():\n",
    "    if key in embedding_idx.keys():\n",
    "        vector = embedding_idx[key]\n",
    "        embedding_mtx[value] = vector\n",
    "        counter += 1\n",
    "        \n",
    "print(f\"{counter} out of {len(embedding_mtx)} vectors were placed into matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>MODELING...</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>NOTE: </b>Embedding layer is one of the available layers in Keras. This is mainly used in Natural Language Processing related applications such as language modeling, but it can also be used with other tasks that involve neural networks. While dealing with NLP problems, we can use pre-trained word embeddings such as GloVe. Alternatively we can also train our own embeddings using Keras embedding layer.</p>\n",
    "<img src=\"img/8.png\" alt=\"LSTM sample model diagram\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating embedding layer\n",
    "# link for info about embedding layers: https://medium.com/analytics-vidhya/understanding-embedding-layer-in-keras-bbe3ff1327ce\n",
    "emb_layer = tf.keras.layers.Embedding(vocabSize, EMBEDDING_DIMENSION, \n",
    "                                      weights=[embedding_mtx],\n",
    "                                      input_length=MAX_SEQ_LENGTH, \n",
    "                                      trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>NOTE: </b>we will be using Long short-term memory (LSTM). Link for a refrence is provided below.</p>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Long_short-term_memory\">Wikipedia page</a>\n",
    "<br /><br /><b>Architecture: </b>\n",
    "<ul>\n",
    "    <li>Embedding Layer - <a href=\"https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526\">read more...</a></li>\n",
    "    <li>Conv1DLayer - <a href=\"\">read more...</a></li>\n",
    "    <li>LSTM - <a href=\"https://towardsdatascience.com/understanding-1d-and-3d-convolution-neural-network-keras-9d8f76e29610\">read more...</a></li>\n",
    "    <li>Dense - <a href=\"https://heartbeat.fritz.ai/classification-with-tensorflow-and-dense-neural-networks-8299327a818a\">read more...</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilizing our model\n",
    "seq_input = Input(shape=(MAX_SEQ_LENGTH,), dtype='int32')\n",
    "emb_sequence = emb_layer(seq_input)\n",
    "x =  SpatialDropout1D(0.2)(emb_sequence)\n",
    "x = Conv1D(64, 5, activation='relu')(x)\n",
    "x = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = tf.keras.Model(seq_input, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>NOTE: </b>we're using Adam optimization algorithm for Gradient Descent.</p>\n",
    "<a href=\"https://keras.io/api/optimizers/adam/\">read more...</a>\n",
    "<p><b>NOTE: </b>we'll be using 'LRScheduler' (<a href=\"https://keras.io/api/callbacks/learning_rate_scheduler/\">read more...</a>) and 'ModelCheckPoint' (<a href=\"https://keras.io/api/callbacks/model_checkpoint/\">read more...</a>) while we training our model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling our model\n",
    "LR = 1e-3\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=LR), \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "ReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n",
    "                                     min_lr=0.01,\n",
    "                                     monitor='val_loss',\n",
    "                                     verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TRAINING...</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if we will be using GPU or CPU to train our model\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(f\"Using GPU to train model...\")\n",
    "else:\n",
    "    print(f\"Using CPU to train model...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, y_test.shape, X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training our model\n",
    "BATCH_SIZE = 1000\n",
    "EPOCHS = 1\n",
    "\n",
    "hist = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                validation_data=(X_test, y_test), callbacks=[ReduceLROnPlateau])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>MODEL METRICS...</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
